import os
from pathlib import Path

import streamlit as st
from opencc import OpenCC  # type: ignore

from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory
from langchain_community.chat_models import ChatOllama

# Constants
cc = OpenCC("s2twp.json")
TMP_DIR = Path(__file__).resolve().parent.joinpath("data", "tmp")
LOCAL_VECTOR_STORE_DIR = Path(__file__).resolve().parent.joinpath("data", "vector_store")
PDF_DATA_DIR = Path(__file__).resolve().parent.joinpath("data", "pdfdata")

# Create necessary directories
if not TMP_DIR.exists():
    TMP_DIR.mkdir(parents=True)

# Streamlit setup
st.set_page_config(page_title="PDF RAG QA ç³»çµ±", page_icon="ğŸ“š", layout="wide")
st.title("äºä»•ä¸¹ RAG QA å±•ç¤ºç³»çµ±")

# Sidebar for model selection
mode = st.sidebar.radio("æ¨¡å‹é¸æ“‡", ("Llama", "openAI"))

def load_documents():
    loader = PyPDFDirectoryLoader(path=TMP_DIR.as_posix(), glob="**/*.pdf")
    return loader.load()

def split_documents(documents):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)
    return text_splitter.split_documents(documents)

def create_embeddings(texts):
    model_name = "BAAI/bge-m3"
    embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={"device": "cpu"})
    vectordb = Chroma.from_documents(texts, embedding=embeddings, persist_directory=LOCAL_VECTOR_STORE_DIR.as_posix())
    return vectordb.as_retriever(search_kwargs={"k": 7})

def define_llm():
    if mode == "Llama":
        return ChatOllama(model="llama3")
    elif mode == "openAI":
        return ChatOpenAI(model="gpt-4o")

def get_prompt(query):
    # prompt_template = """
    # ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„é†«ç™‚å™¨æå°ˆå®¶ï¼Œä½ çš„å›ç­”çš†æ˜¯åŸºæ–¼çµ¦äºˆçš„æ–‡ä»¶è³‡è¨Šï¼Œä¸¦ä¸”ç¢ºä¿å›ç­”æ˜¯æ­£ç¢ºçš„ã€‚ 
    # è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ä»¥ä¸‹å•é¡Œï¼Œè«‹åªè¦å›ç­”å•é¡Œå°±å¥½ã€‚ç¢ºä¿ç­”æ¡ˆå…·æœ‰æ„ç¾©ã€ç›¸é—œæ€§å’Œç°¡æ½”æ€§ï¼š
    # """
    prompt_template = """
    ä½ æ˜¯ä¸€å€‹èƒ½å¤ ç«‹å³ä¸”æº–ç¢ºåœ°å›ç­”ä»»ä½•è«‹æ±‚çš„äººã€‚è«‹ç”¨ä¸­æ–‡å›ç­”ä»¥ä¸‹å•é¡Œã€‚ç¢ºä¿ç­”æ¡ˆå…§å®¹ç¬¦åˆæä¾›çš„è³‡æ–™ä¸”ç°¡è¦æ­£ç¢ºï¼š \n
    """
    return prompt_template + query

def query_llm(retriever, query):
    prompt = get_prompt(query)
    llm = define_llm()
    qa_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, return_source_documents=True)
    result = qa_chain.invoke({"question": prompt, "chat_history": st.session_state.messages})["answer"]
    st.session_state.messages.append((query, result))
    return result

def process_documents():
    try:
        if any(PDF_DATA_DIR.glob("**/*.pdf")):
            loader = PyPDFDirectoryLoader(path=PDF_DATA_DIR.as_posix(), glob="**/*.pdf")
            documents = loader.load()
            texts = split_documents(documents)
            st.session_state.retriever = create_embeddings(texts)
            st.success("è³‡æ–™åº«å·²æ›´æ–°")
        else:
            st.info("æ²’æœ‰æ–°çš„ PDF æ–‡ä»¶")
    except Exception as e:
        st.error(f"è™•ç†æ–‡ä»¶æ™‚å‡ºç¾éŒ¯èª¤: {e}")

def initialize_session():
    if "messages" not in st.session_state:
        process_documents()
        st.session_state.messages = []

def display_chat_history():
    for user_msg, ai_msg in st.session_state.messages:
        st.chat_message("User").write(user_msg)
        st.chat_message("AI").write(ai_msg)

def main():
    initialize_session()
    display_chat_history()
    if query := st.chat_input():
        st.chat_message("User").write(query)
        if "retriever" in st.session_state: # è¦ç¢ºå®šæœ‰è·‘é€²ä¾†
            response = query_llm(st.session_state.retriever, query)
        else:
            response = "è³‡æ–™åº«å°šæœªå»ºç«‹"
        st.chat_message("AI").write(cc.convert(response))

if __name__ == "__main__":
    main()
